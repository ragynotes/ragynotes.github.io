for myself and for others who might be interested in the futurecurrently all pdfs are in the tracked `.gitignore` because large sizes are causing too much grief. 

github does not allow files >100MB

later, will have to address this more robustly. 

to investigate: `git-LFS`in the dev environment the title for http://localhost:1313/about/ is "About**s**". 

Once built, check if this is happening and why.info: [Table of Contents | Hugo](https://gohugo.io/content-management/toc/)in the page archetype I have been working on tonight, I have made use of the emoji `❗` to indicate everything that needs to be done to the document. can probably make a github action to prohibit publication of any file containing one of these as a check. 

[files for archetype](https://github.com/ragynotes/ragynotes.github.io/tree/main/archetypes/document)email, form or whatevermust be non creepy. no google.

see: [Open Source Google Analytics](https://alternativeto.net/software/google-analytics/?license=opensource)

**option 1: goatcounter**

[arp242/goatcounter: Easy web analytics. No tracking of personal data.](https://github.com/arp242/goatcounter)

> There are two ways to run this: as **hosted service** on [goatcounter.com](https://www.goatcounter.com), *free* for non-commercial use, or run it on your own server. The source code is completely Open Source/Free Software, and it can be self-hosted without restrictions.
>
> See [docs/rationale.markdown](https://github.com/arp242/goatcounter/blob/master/docs/rationale.markdown) for some more details on the *"why?"* of this project....
>
> GoatCounter is sponsored by a grant from [NLnet's NGI Zero PET fund](https://nlnet.nl/project/GoatCounter/).

> **Privacy-aware**; doesn’t track users with unique identifiers and doesn't need a GDPR notice. Fine-grained **control over which data is collected**. Also see the [privacy policy](https://www.goatcounter.com/privacy) and [GDPR consent notices](https://www.goatcounter.com/gdpr).

> Integrate on your site with just a **single script tag**:

**option 2: counter.dev**

[ihucos/counter.dev: Web Analytics made simple](https://github.com/ihucos/counter.dev)

> This project aim is to serve many users for free in a sustainable way.

> Usually web analytics solutions track every page loaded. We only track  the first page the user views, this is again more privacy-friendly and  additionally also results in substantial less HTTP requests the server  has to handle. 

- don't love this; would prefer to know exact pages that people visit even if I don't know who it isprobably should have done this prior to starting project...

this is regarding secondary and tertiary sources as opposed to this website which is entirely primary. 

will need some sort of structure with which to organize leads for further documents wanted.Right now the structure of a page bundle created by running the command

```sh
hugo new --kind document content/Collected-Documents/document-title
```
is:

```
 images
 bibliography.bib
 bibliography.json
 bibliography.yaml
 commentary.md
 document.md
 index.md
 modifications.md
 origin.md
```

everything except for `index.md` should be generated by zotero. 
these were set up but now are default. can't find any evidence of their having been set up though I am certain when I started work today it was normal

- [ ] custom favicon
- [ ] hide text at top of pagea [zotero forum thread from 2015](https://forums.zotero.org/discussion/comment/230530#Comment_230530) describes why this is/was turned off and also a workaround for automatically downloading PDFs in the futurenow that I've spent a couple of hours manually changing hidden preferences in `about:config` for `mdnotes` Zotero extension, I am wondering if this could be made default in a fork as it is actually extremely versatile for outputting zotero to static blogs (and probably other things). 

the code for the extension is here: https://github.com/argenos/zotero-mdnotes/blob/master/content/mdnotes.jsputting `typora-copy-images-to:images` in the `yaml` frontmatter prevents hugo from building. 

is there some way these two can live in peace?see #19.This has proved to be much more difficult than I thought it would be. 

For now I am giving up on it after spending way too long on it yesterday, considering that no one may actually want it. 

Will create one big bibtex and link from everywhere. Later, figure out how to chop up and move the files. and maybe even figure out how to avoid so much duplication in the futureand into.. somewhere elsedecide on and create default landing pageswrite 1-2 paragraphs about what this is and is notonce there is more content, make sure this makes senseto narrow the scope a bit, how about creating the site properly with 10 articles instead of doing the whole thing all at once. I believe this is trivial to implementgithub does not allow individual files >100mb. would ideally catch these prior to pushing by noticing as they are committed. 

possible yay or nay?

see also: #12 # Verify file is complete

1. "Info" tab in Zotero contains all available bibliographic information

* Zotero > Better BibTex (see #7) to create citekeys. 

2. A <100mb PDF is attached
3. A text sidecar is attached - review it; markdown as required
4. Are there significant images? 📌 Establish process for that later. 

# Create `YAML` metadata

 * Right-click on item > `Export item` select `Better Bibtext` *with* `Files` but *not* with *notes*. Save to `ragynotes.github.io/content/Documents_Incoming/FromZotero/ZoteroExport.bib`

 * documentation is a bit thin: [Create a publication - Import from BibTeX | Wowchemy](https://wowchemy.com/docs/content/publications/#import-from-bibtex) needs to be installed with all dependencies

 * the follow command will create a directory structure with the bones of pages in `Documents_Incoming/WorkQueue` 

   ```sh
   cd ragynotes.github.io/
   
   academic import --bibtex content/Documents_Incoming/FromZotero/ZoteroExport/ZoteroExport.bib --publication-dir Documents_Incoming/WorkQueue
   ```

   I will note that the `--publication-dir` value is apparently relative to `content` no matter what. it is extremely confusing to figure out because files go all over the place if you specify paths in a usual way. 

   # move attachments

   The previous command leaves the PDFs where there were in `/FromZotero/`. So they must be carefully moved. 

   

   use `OCRmyPDF` command line utility. 

Full documentation: [documentation](https://ocrmypdf.readthedocs.io/en/)

- [Installing OCRmyPDF](https://ocrmypdf.readthedocs.io/en/latest/installation.html)

Most useful

Batch `OCRmyPDF` for PDFs that have been partly OCRed and you only want to work on the pages without any existing text. 

* in place (will _not_ preserve previous version of PDF)
* recursively (every PDF in the current directory and all directories below) 
* in parallel (work faster)
* _skip_ pages with existing text (see [Common error messages: Page already has text](https://ocrmypdf.readthedocs.io/en/latest/errors.html?highlight=skip-text#page-already-has-text) for rationale and other options)

```sh
find . -name '*.pdf' | parallel --tag -j 2 ocrmypdf --skip-text '{}' '{}'
```

to run the same kind of job on just one specific file:


```sh
ocrmypdf --skip-text SomeFile.pdf
```

Batch `OCRmyPDF` for PDFs that have *not* yet been OCRed *at all* 

* in place (will _not_ preserve previous version of PDF)
* recursively (every PDF in the current directory and all directories below) 
* in parallel (work faster)

```sh
find . -name '*.pdf' | parallel --tag -j 2 ocrmypdf '{}' '{}'
```



Sidecars ([docs](https://ocrmypdf.readthedocs.io/en/latest/cookbook.html#produce-pdf-and-text-file-containing-ocr-text))

> ```
> ocrmypdf --sidecar output.txt input.pdf output.pdf
> ```
>
> Note
>
> The sidecar file contains the **OCR text** found by OCRmyPDF. If the document contains pages that already have text, that text will not appear in the sidecar. If the option `--pages` is used, only those pages on which OCR was performed will be included in the sidecar. If certain pages were skipped because of options like `--skip-big` or `--tesseract-timeout`, those pages will not be in the sidecar.
>
> To extract all text from a PDF, whether generated from OCR or otherwise, use a program like Poppler’s `pdftotext` or `pdfgrep`.